{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using ELMO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will try to explore the ELMO embeddings. Link to the elmo paper: https://arxiv.org/pdf/1802.05365.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also have a look at the elmo tutorial provided by allennlp: https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is heavily inspired from the above tutorial, except that it is in a notebook format for easy reproducibility. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another tutorial explaining ELMO very well: http://jalammar.github.io/illustrated-bert/ (see from word embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elmo in a nutshell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings are quite powerful. They give a dense representation (an embedding vector) for each word and these capture relations. Word embeddings are conceptually simple, given a word, it is converted to an embedding vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elmo takes it a step further. Instead of providing word embeddings from the word directly, it instead looks at the whole sentence before giving the embedding. These are called contextualized embeddings since we are looking at the whole context before deciding the embedding of a particular word. Internally, it uses a bidirectional LSTM Language Model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Contextualized Embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contextualized embeddings are important because words by themselves are ambiguous and their meaning becomes clear with their usage. For eg. consider the two sentences: \n",
    "1. \"When not flying, bats hang upside down from their feet, a posture known as roosting.\"\n",
    "2. \"Bats are made of either wood, or a metal alloy (typically aluminum).\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given just the word \"bat\" it is difficult to understand what it means, the animal or a wooden club, but there is no ambiguity when it occurs in the whole sentence. ELMO exploits this knowledge to give richer representations for the words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above case is an example of homography. ELMO also solves the case of polysemy in a similar way. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elmo as a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elmo has three layers. First is the word level representation. Second and third layers are Bi-LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The authors claim that the second layer is good for low-level tasks like POS Tagging. The third layer is good for higher-level tasks like sentiment classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elmo has trainable paramters for weighing each of the layers. So for tasks like POS-Tagging the weight for the second layer is higher and for semantic classification weight for the third layer is higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Elmo in your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are few different ways you might want to use ELMO embeddings depending on your use case:\n",
    "1. Use the elmo embeddings directly in your existing framework.\n",
    "2. Use the elmo embeddings directly, but learn the weight of each layer to compute the final representation\n",
    "3. Fine-tune elmo on the corpus you want to use and then use elmo\n",
    "4. Train elmo along with other pytorch models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Use elmo embeddings directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are essentially two ways to go about this:\n",
    "1. Store the elmo embeddings of the sentences to h5py file.\n",
    "2. Retrieve the embeddings interactively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Storing in a h5py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['You are a bold one.', 'Hello there!', 'Perhaps the archives are incomplete.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sent.txt', 'w') as f:\n",
    "    f.write('\\n'.join(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In your terminal do: `allennlp elmo sent.txt elmo_layers.hdf5 --all`. This should create `elmo_layers.hdf5`. We can open and check it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "h5_file = h5py.File('elmo_layers.hdf5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1', '2', 'sentence_to_index']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k in h5_file.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<HDF5 dataset \"0\": shape (3, 5, 1024), type \"<f4\">,\n",
       " <HDF5 dataset \"1\": shape (3, 2, 1024), type \"<f4\">,\n",
       " <HDF5 dataset \"2\": shape (3, 5, 1024), type \"<f4\">)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5_file['0'], h5_file['1'], h5_file['2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are stored in (BatchSize x SeqLen x EmbeddingDim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Retrieve it interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.commands.elmo import ElmoEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.tokenizers.word_tokenizer import WordTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/11/2018 15:03:25 - INFO - allennlp.commands.elmo -   Initializing ELMo.\n"
     ]
    }
   ],
   "source": [
    "elmo_emb = ElmoEmbedder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "wtokenizer = WordTokenizer()\n",
    "tokenized_sents = wtokenizer.batch_tokenize(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[You, are, a, bold, one, .],\n",
       " [Hello, there, !],\n",
       " [Perhaps, the, archives, are, incomplete, .]]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_sents = [[x.text for x in y] for y in tokenized_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['You', 'are', 'a', 'bold', 'one', '.'],\n",
       " ['Hello', 'there', '!'],\n",
       " ['Perhaps', 'the', 'archives', 'are', 'incomplete', '.']]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/arka/miniconda3/envs/fai/lib/python3.7/site-packages/allennlp/nn/util.py:150: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  index_range = sequence_lengths.new_tensor(torch.arange(0, len(sequence_lengths)))\n"
     ]
    }
   ],
   "source": [
    "vec_elmo = list(elmo_emb.embed_sentences(tok_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 6, 1024), (3, 3, 1024), (3, 6, 1024)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[v.shape for v in vec_elmo]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.61176616, -0.1803728 , -0.6626564 , ...,  0.108252  ,\n",
       "        -0.31069914, -0.76622486],\n",
       "       [-0.03124004,  0.08035831, -0.282419  , ...,  0.03819396,\n",
       "         0.4789119 ,  0.08654939],\n",
       "       [ 0.10400566,  0.12288515, -0.07056469, ..., -0.12283114,\n",
       "        -0.02834528, -0.06579691],\n",
       "       [ 1.1506842 , -0.05340729, -0.30548504, ..., -0.2646592 ,\n",
       "        -0.4776665 ,  0.10963759],\n",
       "       [ 0.31488723, -0.08592107, -0.39453682, ..., -0.66952085,\n",
       "         0.08430362,  0.26585263],\n",
       "       [-0.88715065, -0.20039944, -1.060133  , ..., -0.26554623,\n",
       "         0.21145949,  0.19772954]], dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_elmo[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.61176616, -0.1803728 , -0.6626564 , ...,  0.108252  ,\n",
       "        -0.31069914, -0.76622486],\n",
       "       [-0.03124004,  0.08035831, -0.282419  , ...,  0.03819396,\n",
       "         0.4789119 ,  0.08654939],\n",
       "       [ 0.10400566,  0.12288515, -0.07056469, ..., -0.12283114,\n",
       "        -0.02834528, -0.06579691],\n",
       "       [ 1.1506842 , -0.05340729, -0.30548504, ..., -0.2646592 ,\n",
       "        -0.4776665 ,  0.10963759],\n",
       "       [-0.543568  , -0.09246673, -1.7003565 , ..., -1.9184163 ,\n",
       "        -0.04922673,  0.6620259 ]], dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5_file['0'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only difference between the command line and this method is that we are tokenizing explicitly in the latter. This gives an extra representation for the punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2. Learning only the layer weights for final representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.elmo import Elmo, batch_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "weight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/11/2018 15:52:15 - INFO - allennlp.modules.elmo -   Initializing ELMo\n"
     ]
    }
   ],
   "source": [
    "elmo = Elmo(options_file, weight_file, 1, dropout=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different parameters are: \n",
    "- (i) options file: the configuration file for elmo \n",
    "- (ii) weights file: to load the pretrained model \n",
    "- (iii) number of representations: for most case 1 suffices when you need to use the elmo embeddings in the input. If you also need to use it in output, make this 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['You', 'are', 'a', 'bold', 'one', '.'],\n",
       " ['Hello', 'there', '!'],\n",
       " ['Perhaps', 'the', 'archives', 'are', 'incomplete', '.']]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we get character level representations. Elmo uses character level convolution in case the word is not in a dictionary, and hence requires character level representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_embs = batch_to_ids(tok_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 6, 50])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[259,  90, 112, 118, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259,  98, 115, 102, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259,  98, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259,  99, 112, 109, 101, 260, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259, 112, 111, 102, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259,  47, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261]],\n",
       "\n",
       "        [[259,  73, 102, 109, 109, 112, 260, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259, 117, 105, 102, 115, 102, 260, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259,  34, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "\n",
       "        [[259,  81, 102, 115, 105,  98, 113, 116, 260, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259, 117, 105, 102, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259,  98, 115, 100, 105, 106, 119, 102, 116, 260, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259,  98, 115, 102, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259, 106, 111, 100, 112, 110, 113, 109, 102, 117, 102, 260, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259,  47, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261]]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/arka/miniconda3/envs/fai/lib/python3.7/site-packages/allennlp/nn/util.py:150: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than tensor.new_tensor(sourceTensor).\n",
      "  index_range = sequence_lengths.new_tensor(torch.arange(0, len(sequence_lengths)))\n"
     ]
    }
   ],
   "source": [
    "emb_elmo = elmo(char_embs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a dict, with keys `elmo_representation` and `mask`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict, dict_keys(['elmo_representations', 'mask']))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(emb_elmo), emb_elmo.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_elmo_vec = emb_elmo['elmo_representations']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a list of length number of representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 1)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(emb_elmo_vec), len(emb_elmo_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_elmo_vec0 = emb_elmo_vec[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 6, 1024])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_elmo_vec0.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mask is simply 1 where a word is present, 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = emb_elmo['mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Fine-tune on a separate corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recommended way is to use this repository: https://github.com/allenai/bilm-tf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular see: https://github.com/allenai/bilm-tf#how-to-do-fine-tune-a-model-on-additional-unlabeled-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Train elmo along with other pytorch models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply set `requires_grad=True` when instantiating the `elmo` model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/11/2018 16:10:31 - INFO - allennlp.modules.elmo -   Initializing ELMo\n"
     ]
    }
   ],
   "source": [
    "elmo = Elmo(options_file, weight_file, 1, dropout=0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows the whole elmo model to be trained."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
