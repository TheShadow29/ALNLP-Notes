{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying out the allennlp tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to the tutorial: https://allennlp.org/tutorials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I try to explore the allennlp library starting from their tutorial. Specifically trying to decode what each and every module does for better understanding their framework and of course for faster prototyping. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Underlined stuff are hyper-linked to docs for easy access. You might also find it easier to simply do ??Module in a new cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### External modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First up is [`typing`](https://docs.python.org/3/library/typing.html). It allows type hints which can be used in functions to denote what the expected input and output type would be analogous to cpp. Some interesting ones are Union (either or) and Callable (another function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing much, just regular pytorch and numpy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up example cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first experiment with the \n",
    "- word 'Hello', \n",
    "- sentence 'We live in a society.' \n",
    "- sentences \\['You are a bold one.', 'Perhaps the archives are incomplete.'\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = 'Hello'\n",
    "sent = 'We live in a society.'\n",
    "sents = ['You are a bold one.', 'Perhaps the archives are incomplete.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start importing allennlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.tokenizers import Token, WordTokenizer\n",
    "from allennlp.data.tokenizers.token import show_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`Token`](https://allenai.github.io/allennlp-docs/api/allennlp.data.tokenizers.html#allennlp.data.tokenizers.token.Token): is a wrapper around a word to keep track of some important stuff like its lemma, or a part of speech tag etc. \n",
    "\n",
    "[`WordTokenizer`](https://allenai.github.io/allennlp-docs/api/allennlp.data.tokenizers.html#word-tokenizer): Tokenizes a sentence and outputs a list of tokens. By default it uses spacy's implementation for tokenizing words.\n",
    "\n",
    "[`show_token`](https://allenai.github.io/allennlp-docs/api/allennlp.data.tokenizers.html#allennlp.data.tokenizers.token.show_token): a convenience function to print your tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_token = Token(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how a single token looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello (idx: None) (lemma: None) (pos: None) (tag: None) (dep: None) (ent_type: None) '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_token(word_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that only the 'text' is filled, others tags are None and get filled up when one does some other processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now tokenize a whole sentence using the WordTokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_toks = WordTokenizer().tokenize(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenized sentence, the output being a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[We, live, in, a, society, .]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_toks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the printed tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We (idx: 0) (lemma: We) (pos: ) (tag: ) (dep: ) (ent_type: ) ',\n",
       " 'live (idx: 3) (lemma: live) (pos: ) (tag: ) (dep: ) (ent_type: ) ',\n",
       " 'in (idx: 8) (lemma: in) (pos: ) (tag: ) (dep: ) (ent_type: ) ',\n",
       " 'a (idx: 11) (lemma: a) (pos: ) (tag: ) (dep: ) (ent_type: ) ',\n",
       " 'society (idx: 13) (lemma: society) (pos: ) (tag: ) (dep: ) (ent_type: ) ',\n",
       " '. (idx: 20) (lemma: .) (pos: ) (tag: ) (dep: ) (ent_type: ) ']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[show_token(s) for s in sent_toks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also process multiple sentences at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_toks = WordTokenizer().batch_tokenize(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[You, are, a, bold, one, .], [Perhaps, the, archives, are, incomplete, .]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['You (idx: 0) (lemma: You) (pos: ) (tag: ) (dep: ) (ent_type: ) ',\n",
       " 'are (idx: 4) (lemma: be) (pos: ) (tag: ) (dep: ) (ent_type: ) ',\n",
       " 'a (idx: 8) (lemma: a) (pos: ) (tag: ) (dep: ) (ent_type: ) ',\n",
       " 'bold (idx: 10) (lemma: bold) (pos: ) (tag: ) (dep: ) (ent_type: ) ',\n",
       " 'one (idx: 15) (lemma: one) (pos: ) (tag: ) (dep: ) (ent_type: ) ',\n",
       " '. (idx: 18) (lemma: .) (pos: ) (tag: ) (dep: ) (ent_type: ) ',\n",
       " 'Perhaps (idx: 0) (lemma: Perhaps) (pos: ) (tag: ) (dep: ) (ent_type: ) ',\n",
       " 'the (idx: 8) (lemma: the) (pos: ) (tag: ) (dep: ) (ent_type: ) ',\n",
       " 'archives (idx: 12) (lemma: archive) (pos: ) (tag: ) (dep: ) (ent_type: ) ',\n",
       " 'are (idx: 21) (lemma: be) (pos: ) (tag: ) (dep: ) (ent_type: ) ',\n",
       " 'incomplete (idx: 25) (lemma: incomplete) (pos: ) (tag: ) (dep: ) (ent_type: ) ',\n",
       " '. (idx: 35) (lemma: .) (pos: ) (tag: ) (dep: ) (ent_type: ) ']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[show_token(s) for snt in sents_toks for s in snt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TokenIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`TokenIndexer`](https://allenai.github.io/allennlp-docs/api/allennlp.data.token_indexers.html#allennlp.data.token_indexers.token_indexer.TokenIndexer): Converts a token or list of tokens to indices. These indices refer to the index of the token in some vocabulary to be used by the model.\n",
    "\n",
    "[`SingleIdTokenIndexer`](https://allenai.github.io/allennlp-docs/api/allennlp.data.token_indexers.html#single-id-token-indexer): Converts a single field "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that the token indexer requires a vocabulary, however, we haven't created one yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = SingleIdTokenIndexer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fields and Instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data import Field\n",
    "from allennlp.data.fields import TextField, SequenceLabelField\n",
    "from allennlp.data import Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`Field`](https://allenai.github.io/allennlp-docs/api/allennlp.data.fields.html#allennlp.data.fields.field.Field): is simply some data to be feeded to the pipeline of your model. Some important use cases are :\n",
    " - tokenized text, for example, the text \"The movie\" would be stored as a tokenized text in a field as \\['The', 'movie'\\].\n",
    " - numerical id for the tokenized text, suppose 'The' maps to 1, and 'movie' maps to 35 in a dictionary of words, the field could contain \\[1, 35\\]. \n",
    " - Also note that field can contain multiple sentences of varying lengths. If you want to pass such a field to your pipeline, it needs to be appropriately padded. Field contains an `as_tensor` method to convert the data into tensor and `batch_tensors` to convert into tensors after appropriate padding. \n",
    "\n",
    "For most purposes you should be able to use one of the ready made Fields like the `TextField` or `SequenceLabelField`.\n",
    " - [`TextField`](https://allenai.github.io/allennlp-docs/api/allennlp.data.fields.html#allennlp.data.fields.text_field.TextField): The field contains tokenized strings. One needs to pass raw strings through a [`tokenizer`](https://allenai.github.io/allennlp-docs/api/allennlp.data.tokenizers.html) before passing it to the field.\n",
    " - [`SequenceLabelField`](https://allenai.github.io/allennlp-docs/api/allennlp.data.fields.html#allennlp.data.fields.sequence_label_field.SequenceLabelField): assigns some label for each element in a field. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`Instance`](https://allenai.github.io/allennlp-docs/api/allennlp.data.instance.html#allennlp.data.instance.Instance): is simply a dictionary mapping with string keys, and values as fields. One data point is one Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets create a TextField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_text_field = TextField(sent_toks, SingleIdTokenIndexer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<allennlp.data.fields.text_field.TextField at 0x7fe2bb542a20>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_text_field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instance_from_tokenized_sent(tok_sent: List[Token]) -> Instance:\n",
    "    \"Converts tokenized sentence into Instances. Each instance being TextField\"\n",
    "    sent_tok_text_field = TextField(tok_sent, {\"tokens\": SingleIdTokenIndexer()})\n",
    "    fields = {'sentence': sent_tok_text_field}\n",
    "    return Instance(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instances_from_tokenized_sents(tok_sents: List[List[Token]]) -> List[Instance]:\n",
    "    \"Converts list of sentences to instances.\"\n",
    "    return [get_instance_from_tokenized_sent(tok_sent) for tok_sent in tok_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_instance = get_instance_from_tokenized_sent(sent_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': [We, live, in, a, society, .],\n",
       " '_token_indexers': {'tokens': <allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer at 0x7fe3208363c8>},\n",
       " '_indexed_tokens': None,\n",
       " '_indexer_name_to_indexed_token': None}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_instance.fields['sentence'].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_instances = get_instances_from_tokenized_sents(sents_toks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'fields': {'sentence': <allennlp.data.fields.text_field.TextField at 0x7fe320917208>},\n",
       "  'indexed': False},\n",
       " {'fields': {'sentence': <allennlp.data.fields.text_field.TextField at 0x7fe2bb52cef0>},\n",
       "  'indexed': False}]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f.__dict__ for f in few_instances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tokens': [You, are, a, bold, one, .],\n",
       "  '_token_indexers': {'tokens': <allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer at 0x7fe320836a90>},\n",
       "  '_indexed_tokens': None,\n",
       "  '_indexer_name_to_indexed_token': None},\n",
       " {'tokens': [Perhaps, the, archives, are, incomplete, .],\n",
       "  '_token_indexers': {'tokens': <allennlp.data.token_indexers.single_id_token_indexer.SingleIdTokenIndexer at 0x7fe2bb52c1d0>},\n",
       "  '_indexed_tokens': None,\n",
       "  '_indexer_name_to_indexed_token': None}]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[f.fields['sentence'].__dict__ for f in few_instances]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.vocabulary import Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`Vocabulary`](https://allenai.github.io/allennlp-docs/api/allennlp.data.vocabulary.html): Provides a mapping from a string to an integer index. This can be created `from_files`, `from_instances`. It is quite useful, since building a dictionary is fundamental to almost all nlp tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have `few_instances` we are now ready to build a vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/08/2018 18:23:17 - INFO - allennlp.data.vocabulary -   Fitting token dictionary from dataset.\n",
      "100%|██████████| 2/2 [00:00<00:00, 9147.88it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab =  Vocabulary.from_instances(few_instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get the vocabulary mapping tokens to indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'@@PADDING@@': 0,\n",
       " '@@UNKNOWN@@': 1,\n",
       " 'are': 2,\n",
       " '.': 3,\n",
       " 'You': 4,\n",
       " 'a': 5,\n",
       " 'bold': 6,\n",
       " 'one': 7,\n",
       " 'Perhaps': 8,\n",
       " 'the': 9,\n",
       " 'archives': 10,\n",
       " 'incomplete': 11}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get_token_to_index_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get index to the word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '@@PADDING@@',\n",
       " 1: '@@UNKNOWN@@',\n",
       " 2: 'are',\n",
       " 3: '.',\n",
       " 4: 'You',\n",
       " 5: 'a',\n",
       " 6: 'bold',\n",
       " 7: 'one',\n",
       " 8: 'Perhaps',\n",
       " 9: 'the',\n",
       " 10: 'archives',\n",
       " 11: 'incomplete'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get_index_to_token_vocabulary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get just the index for a word or just the word given index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'are'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get_token_from_index(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get_token_index('are')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get some statistics about the vocabulary created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/08/2018 18:25:37 - INFO - allennlp.data.vocabulary -   Printed vocabulary statistics are only for the part of the vocabulary generated from instances. If vocabulary is constructed by extending saved vocabulary with dataset instances, the directly loaded portion won't be considered here.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "----Vocabulary Statistics----\n",
      "\n",
      "\n",
      "Top 10 most frequent tokens in namespace 'tokens':\n",
      "\tToken: are\t\tFrequency: 2\n",
      "\tToken: .\t\tFrequency: 2\n",
      "\tToken: You\t\tFrequency: 1\n",
      "\tToken: a\t\tFrequency: 1\n",
      "\tToken: bold\t\tFrequency: 1\n",
      "\tToken: one\t\tFrequency: 1\n",
      "\tToken: Perhaps\t\tFrequency: 1\n",
      "\tToken: the\t\tFrequency: 1\n",
      "\tToken: archives\t\tFrequency: 1\n",
      "\tToken: incomplete\t\tFrequency: 1\n",
      "\n",
      "Top 10 longest tokens in namespace 'tokens':\n",
      "\tToken: incomplete\t\tlength: 10\tFrequency: 1\n",
      "\tToken: archives\t\tlength: 8\tFrequency: 1\n",
      "\tToken: Perhaps\t\tlength: 7\tFrequency: 1\n",
      "\tToken: bold\t\tlength: 4\tFrequency: 1\n",
      "\tToken: are\t\tlength: 3\tFrequency: 2\n",
      "\tToken: You\t\tlength: 3\tFrequency: 1\n",
      "\tToken: one\t\tlength: 3\tFrequency: 1\n",
      "\tToken: the\t\tlength: 3\tFrequency: 1\n",
      "\tToken: .\t\tlength: 1\tFrequency: 2\n",
      "\tToken: a\t\tlength: 1\tFrequency: 1\n",
      "\n",
      "Top 10 shortest tokens in namespace 'tokens':\n",
      "\tToken: a\t\tlength: 1\tFrequency: 1\n",
      "\tToken: .\t\tlength: 1\tFrequency: 2\n",
      "\tToken: the\t\tlength: 3\tFrequency: 1\n",
      "\tToken: one\t\tlength: 3\tFrequency: 1\n",
      "\tToken: You\t\tlength: 3\tFrequency: 1\n",
      "\tToken: are\t\tlength: 3\tFrequency: 2\n",
      "\tToken: bold\t\tlength: 4\tFrequency: 1\n",
      "\tToken: Perhaps\t\tlength: 7\tFrequency: 1\n",
      "\tToken: archives\t\tlength: 8\tFrequency: 1\n",
      "\tToken: incomplete\t\tlength: 10\tFrequency: 1\n"
     ]
    }
   ],
   "source": [
    "vocab.print_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### File Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[`cached_path`](https://allenai.github.io/allennlp-docs/api/allennlp.common.file_utils.html#allennlp.common.file_utils.cached_path): A convenience function taking either an url or a localpath. If url downloads the file to some localpath, if localpath, ensures that it exists. Returns the cached localpath back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.common.file_utils import cached_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = cached_path(\n",
    "    'https://raw.githubusercontent.com/allenai/allennlp'\n",
    "    '/master/tutorials/tagger/training.txt')\n",
    "validation_dataset_path = cached_path(\n",
    "    'https://raw.githubusercontent.com/allenai/allennlp'\n",
    "    '/master/tutorials/tagger/validation.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can open the file and see the format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_dataset_path, 'r') as f:\n",
    "    lines = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The###DET dog###NN ate###V the###DET apple###NN\\n',\n",
       " 'Everybody###NN read###V that###DET book###NN\\n']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataSet Readers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A superclass for all dataset readers. Has a method to read, and convert text to instance. Both need to be implemented in case of a custom dataset. Lazy defines whether or not to input the whole dataset at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.data.dataset_readers import DatasetReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a POS Tagger. Note that the `_read` function returns an Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosDatasetReader(DatasetReader):\n",
    "    \"\"\"\n",
    "    DatasetReader for PoS tagging data, one sentence per line, like\n",
    "\n",
    "        The###DET dog###NN ate###V the###DET apple###NN\n",
    "    \"\"\"\n",
    "    def __init__(self, token_indexers: Dict[str, TokenIndexer] = None) -> None:\n",
    "        super().__init__(lazy=False)\n",
    "        self.token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "    def text_to_instance(self, tokens: List[Token], tags: List[str] = None) -> Instance:\n",
    "        sentence_field = TextField(tokens, self.token_indexers)\n",
    "        fields = {\"sentence\": sentence_field}\n",
    "\n",
    "        if tags:\n",
    "            label_field = SequenceLabelField(labels=tags, sequence_field=sentence_field)\n",
    "            fields[\"labels\"] = label_field\n",
    "\n",
    "        return Instance(fields)\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        with open(file_path) as f:\n",
    "            for line in f:\n",
    "                pairs = line.strip().split()\n",
    "                sentence, tags = zip(*(pair.split(\"###\") for pair in pairs))\n",
    "                yield self.text_to_instance([Token(word) for word in sentence], tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.training.trainer import Trainer\n",
    "from allennlp.predictors import SentenceTaggerPredictor\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LstmTagger(Model):\n",
    "    def __init__(self,\n",
    "                 word_embeddings: TextFieldEmbedder,\n",
    "                 encoder: Seq2SeqEncoder,\n",
    "                 vocab: Vocabulary) -> None:\n",
    "        super().__init__(vocab)\n",
    "        self.word_embeddings = word_embeddings\n",
    "        self.encoder = encoder\n",
    "        self.hidden2tag = torch.nn.Linear(in_features=encoder.get_output_dim(),\n",
    "                                          out_features=vocab.get_vocab_size('labels'))\n",
    "        self.accuracy = CategoricalAccuracy()\n",
    "    def forward(self,\n",
    "                sentence: Dict[str, torch.Tensor],\n",
    "                labels: torch.Tensor = None) -> torch.Tensor:\n",
    "        mask = get_text_field_mask(sentence)\n",
    "        embeddings = self.word_embeddings(sentence)\n",
    "        encoder_out = self.encoder(embeddings, mask)\n",
    "        tag_logits = self.hidden2tag(encoder_out)\n",
    "        output = {\"tag_logits\": tag_logits}\n",
    "        if labels is not None:\n",
    "            self.accuracy(tag_logits, labels, mask)\n",
    "            output[\"loss\"] = sequence_cross_entropy_with_logits(tag_logits, labels, mask)\n",
    "\n",
    "        return output\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        return {\"accuracy\": self.accuracy.get_metric(reset)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PosDatasetReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a3 = a1['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a3.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = a1['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a4 = a2.tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.get_token_to_index_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset[0]['sentence'].tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocabulary.from_instances(train_dataset + validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'),\n",
    "                            embedding_dim=EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.get_vocab_size('labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??BasicTextFieldEmbedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings._token_embedders.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = PytorchSeq2SeqWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, batch_first=True))\n",
    "model = LstmTagger(word_embeddings, lstm, vocab)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "iterator = BucketIterator(batch_size=2, sorting_keys=[(\"sentence\", \"num_tokens\")])\n",
    "iterator.index_with(vocab)\n",
    "trainer = Trainer(model=model,\n",
    "                  optimizer=optimizer,\n",
    "                  iterator=iterator,\n",
    "                  train_dataset=train_dataset,\n",
    "                  validation_dataset=validation_dataset,\n",
    "                  patience=10,\n",
    "                  num_epochs=1000)\n",
    "trainer.train()\n",
    "predictor = SentenceTaggerPredictor(model, dataset_reader=reader)\n",
    "tag_logits = predictor.predict(\"The dog ate the apple\")['tag_logits']\n",
    "tag_ids = np.argmax(tag_logits, axis=-1)\n",
    "print([model.vocab.get_token_from_index(i, 'labels') for i in tag_ids])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
